Question:1
A)
agent1.sources =source1
agent1.sinks = sink1
agent1.channels = channel1

agent1.sources.source1.channels = channel1
agent1.sinks.sink1.channel = channel1

agent1.sources.source1.type = org.apache.flume.source.twitter.TwitterSource
agent1.sources.source1.conkey = iiKyikoYrxzrCQ5ndKMicrpY9
agent1.sources.source1.conSec = kF0MOvuGSe2eLJP79QqFGaoTrOGQFC0jRHLdCnprWO9t4FG9xQ
agent1.sources.source1.accessToken = 1493966103009005442-f3PXFKEweDPLxu5HZOszZNxlteKLiV
agent1.sources.source1.accessTokenSecret = Dfo2uMu7mZGX31JIzQs2br2T7G1PKShMxaN1pS5fVJ34i  
agent1.sources.source1.keywords = covid-19

agent1.sinks.sink1.type = hdfs
agent1.sinks.sink1.hdfs.path = /flume/twitter
agent1.sinks.sink1.hdfs.filePrefix = events
agent1.sinks.sink1.hdfs.fileSuffix = .log
agent1.sinks.sink1.hdfs.inUsePrefix = _
agent1.sinks.sink1.hdfs.fileType = DataStream

agent1.channels.channel1.type = memory
agent1.channels.channel1.capacity = 1000

Question:2
A)  
sqoop import\
-- connect jdbc:mysql://localhost:3306/PetsDb \
--username=root \
--password=password \
-- table=pet \
--hive-import \
--hive-table=pet_direct \
--target-dir/mysql/table/pet_direct\
--m1

Question:3
A)
from pyspark.sql import Row
flightsPath = "hdfs:///spark/rdd/flights.csv"
flightsData = sc.textFile(flightsPath)

flightsData.first()
blanks = flightsDat.map(lambda x:','.join(x or '00.00' for x in x.split(',')))
blanks.take(10)

blanks.count()
blanks.collect()


Question-4:
A)
twitterPath = "hdfs:///spark/rdd/cache-0.json"
import json
twitterData = sc.textFile(twitterPath).map(lambda x:json.loads(x))
twitterData.take(1)
twitterData.filter(lambda x:x['user']['screen_name']=='realDonaldTrump').map(lambda x:x['text']).take(10)
from pyspark import SQLContext,Row
sqlC = SQLContext(sc)
sqlC
twitterTable= sqlC.read.json(twitterPath)
twitterTable.registerTempTable("twitterTab")
sqlC.sql("select text,user.screen_name from twitterTab where user.screen_name='realDonaldTrump' limit 10").collect()






Question:5
A)
from pyspark import SparkContext
from pyspark.streaming import StreamContext

sc = SparkContext("local[2]","streamErrorCount")
ssc = StreamingContext(sc,10)

ssc.checkpoint("hdfs:///spark/streaming")
ds1 = ssc.socketTextStream("localhost",9999)
count = ds1.flatMap(lambda x:x.split(" ").filter(lambda word: "Error" in word).map(lambda word:(word,1)).reduceByKey(lambda x,y:x+y)

count.pprint()
ssc.start()
ssc.awaitTermination()

Question:6
A)
airportDelays = fp.map(lambda x:(x.origin,x.dep_delay))
airportDelays.keys().take(10)
airportTotalDelay = airportDelays.reduceByKey(lambda x,y:x+y)
airportTotalDelay.take(10)
airportsSumCount = airportTotalDelay.join(airportCount)
airportsSumCount.take(10)
airportAvgDelay=airportsSumCount.mapValues(lambda x:x[0]/float(x[1]))
airportsAvgDelay.take(10)
airportAvgDelay.sortBy(lambda x:-x[1]).take(10)
